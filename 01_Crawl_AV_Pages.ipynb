{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5MeY1g961FdNRUNr/F5sf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genaiconference/Agentic_KAG/blob/main/01_Crawl_AV_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AV-crawl: Advanced Web Crawler with Filtering and Markdown Conversion\n",
        "\n",
        "#### Web crawling: Crawls, filters URLs, and converts HTML to Markdown.\n"
      ],
      "metadata": {
        "id": "GdlnV-TIyC5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/genaiconference/Agentic_KAG.git"
      ],
      "metadata": {
        "id": "5zJpdD4MDpH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "This notebook showcases a practical web-crawling workflow:\n",
        "\n",
        "- Crawl **internal** links from a starting URL (here: Analytics Vidhya’s DataHack Summit site)\n",
        "- Filter crawled URLs based on inclusion/exclusion patterns or domains\n",
        "- Convert raw HTML content into clean Markdown text for easier reading or exporting\n"
      ],
      "metadata": {
        "id": "w3Z7oEK9yMUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Requirements & Setup\n",
        "\n",
        "First, install dependencies:\n",
        "- `requests` and `beautifulsoup4` for crawling\n",
        "- `markdownify` (easy, tested) or `html-to-markdown` (modern alternative) for HTML-to-Markdown\n",
        "\n"
      ],
      "metadata": {
        "id": "e9wi09dyyQyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install r-r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzAstnw5yaCT",
        "outputId": "70aad13b-ec59-4fdf-cea4-0a406d8a2d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4==4.13.4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: html2text==2025.4.15 in /usr/local/lib/python3.11/dist-packages (2025.4.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4==4.13.4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4==4.13.4) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from collections import deque\n",
        "import time\n",
        "import warnings\n",
        "import urllib3\n",
        "\n",
        "# Optional: Try html-to-markdown if preferred (pip install html-to-markdown)\n",
        "try:\n",
        "    from markdownify import markdownify as md_convert\n",
        "except ImportError:\n",
        "    # fallback to html-to-markdown if installed\n",
        "    try:\n",
        "        from html_to_markdown import convert_to_markdown as md_convert\n",
        "    except ImportError:\n",
        "        md_convert = None\n",
        "\n",
        "# Suppress warnings for notebook clarity\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "IUKNI4nkyi4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Crawler Configuration and Utility Functions"
      ],
      "metadata": {
        "id": "LLXMh5x_yn3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_URL = \"https://www.analyticsvidhya.com/datahacksummit/\"\n",
        "BASE_DOMAIN = \"www.analyticsvidhya.com\"\n",
        "MAX_PAGES = 100\n",
        "\n",
        "visited = set()\n",
        "queue = deque([START_URL])\n",
        "output_urls = []\n",
        "page_data = []  # List of dicts: {'url', 'content'}\n"
      ],
      "metadata": {
        "id": "V1LHLh7GyrA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_url(url):\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
        "\n",
        "def is_internal_url(url, base_domain=BASE_DOMAIN):\n",
        "    return urlparse(url).netloc == base_domain\n",
        "\n",
        "def url_passes_filters(url, include_patterns=None, exclude_patterns=None, domain_blocklist=None):\n",
        "    \"\"\"\n",
        "    Returns True if the URL passes filtering rules (inclusion, exclusion, blocklist).\n",
        "    Patterns: lists of strings (keywords, domain names, substrings, etc.)\n",
        "    \"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    # Block specific domains\n",
        "    if domain_blocklist:\n",
        "        for dom in domain_blocklist:\n",
        "            if dom in parsed.netloc:\n",
        "                return False\n",
        "    # Block by substring pattern\n",
        "    if exclude_patterns:\n",
        "        for p in exclude_patterns:\n",
        "            if p in url:\n",
        "                return False\n",
        "    # Allow only certain substrings if include_patterns specified\n",
        "    if include_patterns:\n",
        "        for p in include_patterns:\n",
        "            if p in url:\n",
        "                return True\n",
        "        return False\n",
        "    return True  # if nothing triggered above\n"
      ],
      "metadata": {
        "id": "nDBRPHh9ytrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Web Crawler With Filtering\n",
        "\n",
        "Demo shows filtering: blocklist of unwanted domains & substrings, and/or allowlist (include patterns).\n"
      ],
      "metadata": {
        "id": "7LMqw7Q2ywZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Block social or checkout domains, and allow only pages with 'datahack' in the path\n",
        "DOMAIN_BLOCKLIST = ['facebook.com', 'linkedin.com', 'twitter.com', 'instagram.com', 'explara.com']\n",
        "EXCLUDE_PATTERNS = ['privacy', 'policy', 'checkout', 'login']  # Exclusion substrings\n",
        "INCLUDE_PATTERNS = None  # Example: ['datahack']\n",
        "\n",
        "def crawl():\n",
        "    pages_crawled = 0\n",
        "    while queue and pages_crawled < MAX_PAGES:\n",
        "        current_url = queue.popleft()\n",
        "        if current_url in visited or not url_passes_filters(\n",
        "            current_url, INCLUDE_PATTERNS, EXCLUDE_PATTERNS, DOMAIN_BLOCKLIST):\n",
        "            continue\n",
        "        visited.add(current_url)\n",
        "        pages_crawled += 1\n",
        "        print(f\"[{pages_crawled}] Crawling: {current_url}\")\n",
        "        try:\n",
        "            response = requests.get(current_url, timeout=8, verify=False)\n",
        "            response.raise_for_status()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {e}\")\n",
        "            continue\n",
        "\n",
        "        output_urls.append(current_url)\n",
        "        page_data.append({'url': current_url, 'content': response.text})\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        for a_tag in soup.find_all(\"a\", href=True):\n",
        "            href = a_tag[\"href\"]\n",
        "            full_url = urljoin(current_url, href)\n",
        "            normalized_url = full_url.split(\"#\")[0].rstrip(\"/\")\n",
        "            if (\n",
        "                is_valid_url(normalized_url)\n",
        "                and is_internal_url(normalized_url)\n",
        "                and normalized_url not in visited\n",
        "            ):\n",
        "                if url_passes_filters(normalized_url, INCLUDE_PATTERNS, EXCLUDE_PATTERNS, DOMAIN_BLOCKLIST):\n",
        "                    queue.append(normalized_url)\n",
        "    print(f\"\\n✅ {len(output_urls)} pages crawled after filtering.\")\n"
      ],
      "metadata": {
        "id": "vjOZcB7dyzp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the crawler\n",
        "crawl()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF9OjYEhy2iP",
        "outputId": "8d130bef-0bdf-4954-9680-ad20f1d97bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Crawling: https://www.analyticsvidhya.com/datahacksummit/\n",
            "[2] Crawling: https://www.analyticsvidhya.com/datahacksummit\n",
            "[3] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/awards\n",
            "[4] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/mastering-intelligent-agents-a-deep-dive-into-agentic-ai-building-ai\n",
            "[5] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/build-a-production-ready-multi-agent-application-with-crewai\n",
            "[6] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/agentic-rag-workshop-from-fundamentals-to-real-world-implemenno-title\n",
            "[7] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/llmops-productionalizing-real-world-applications-with-llms-2\n",
            "[8] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/generative-ai-for-business-leaders\n",
            "[9] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/agentops-building-and-deploying-ai-agents\n",
            "[10] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/from-theory-to-practice-training-llms-reinforcement-learning-and-ai\n",
            "[11] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/building-a-multimodal-telegram-agent-that-sees-talks-and-thinks\n",
            "[12] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/mastering-llms-training-fine-tuning-and-best-practices-2\n",
            "[13] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/workshops/mastering-real-world-agentic-ai-applications-with-ag2-autogen\n",
            "[14] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/pratyush-kumar\n",
            "[15] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/manish-gupta-2\n",
            "[16] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/geetha-manjunath\n",
            "[17] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/srikanth-velamakanni-2\n",
            "[18] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/joshua-starmer-2\n",
            "[19] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/luis-serrano\n",
            "[20] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/miguel-otero-pedrido\n",
            "[21] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/qingyun-wu\n",
            "[22] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers\n",
            "[23] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sponsors/dentsu-global-service\n",
            "[24] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sponsors/american-express\n",
            "[25] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sponsors/quanhack\n",
            "[26] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sponsors/tiger-analytics\n",
            "Failed: 500 Server Error: Internal Server Error for url: https://www.analyticsvidhya.com/datahacksummit-2025/sponsors/tiger-analytics\n",
            "[27] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sponsors\n",
            "[28] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/build-a-production-ready-multi-agent-application-with-crewai\n",
            "[29] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/agentic-rag-workshop-from-fundamentals-to-real-world-implemenno-title\n",
            "[30] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/generative-ai-for-business-leaders\n",
            "[31] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/llmops-productionalizing-real-world-applications-with-llms-2\n",
            "[32] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/mastering-real-world-agentic-ai-applications-with-ag2-autogen\n",
            "[33] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/from-theory-to-practice-training-llms-reinforcement-learning-and-ai\n",
            "[34] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/building-a-multimodal-telegram-agent-that-sees-talks-and-thinks\n",
            "[35] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/agentops-building-and-deploying-ai-agents\n",
            "[36] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/mastering-intelligent-agents-a-deep-dive-into-agentic-ai-building-ai\n",
            "[37] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/mastering-llms-training-fine-tuning-and-best-practices-2\n",
            "[38] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/building-indias-ai-ecosystem-from-vision-to-sovereignty\n",
            "[39] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/responsible-ai-in-medical-imaging-a-case-study\n",
            "[40] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/keynote-session\n",
            "[41] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/keynote-session-2\n",
            "[42] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/a-visual-guide-to-attention-mechanism-in-llms\n",
            "[43] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/why-genai-and-llms-fail-and-how-fine-tuning-helps-them\n",
            "[44] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions\n",
            "[45] Crawling: https://www.analyticsvidhya.com/cdn-cgi/l/email-protection\n",
            "[46] Crawling: https://www.analyticsvidhya.com/datahacksummit/.\n",
            "[47] Crawling: https://www.analyticsvidhya.com/datahacksummit-2024\n",
            "[48] Crawling: https://www.analyticsvidhya.com/dhs-2023\n",
            "Failed: 403 Client Error: Forbidden for url: https://www.analyticsvidhya.com/dhs-2023\n",
            "[49] Crawling: https://www.analyticsvidhya.com/datahack-summit-2019\n",
            "[50] Crawling: https://www.analyticsvidhya.com/datahack-summit-2018\n",
            "[51] Crawling: https://www.analyticsvidhya.com/datahack-summit-2017\n",
            "[52] Crawling: https://www.analyticsvidhya.com/terms\n",
            "[53] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/dipanjan-sarkar\n",
            "[54] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/alessandro-romano\n",
            "[55] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/arun-prakash-asokan-2\n",
            "[56] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/kartik-nighania-2\n",
            "[57] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/david-zakkam\n",
            "[58] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/bhaskarjit-sarmah-2\n",
            "[59] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/raghav-bali-2\n",
            "[60] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/quantifying-our-confidence-in-neural-networks-and-ai\n",
            "[61] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/beyond-pocs-building-real-world-agentic-systems\n",
            "[62] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/sessions/agentify-go-to-market-build-sales-marketing-ai-agents-with-mcp\n",
            "[63] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/krishna-kumar-tiwari\n",
            "[64] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/ranjani-mani\n",
            "[65] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/rohan-rao-2\n",
            "[66] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/anand-s-2\n",
            "[67] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/kunal-jain-2\n",
            "[68] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/tanika-gupta-2\n",
            "[69] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/dr-kiran-r-2\n",
            "[70] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/syed-quiser-ahmed\n",
            "[71] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/gauri-kholkar\n",
            "[72] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/mohsin-hasan-khan\n",
            "[73] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/kuldeep-jiwani-2\n",
            "[74] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/vijay-gabale-2\n",
            "[75] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/praveen-kumar-gs\n",
            "[76] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/ruchi-awasthi\n",
            "[77] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/harshad-khadilkar-2\n",
            "[78] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/jayita-bhattacharyya-2\n",
            "[79] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/priyanka-choudhary\n",
            "[80] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/shubhradeep-nandi\n",
            "[81] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/satnam-singh-2\n",
            "[82] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/purva-porwal\n",
            "[83] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/manoranjan-rajguru-2\n",
            "[84] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/nitin-agarwal\n",
            "[85] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/rutvik-acharya\n",
            "[86] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/sanathraj-narayan-2\n",
            "[87] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/vignesh-kumar\n",
            "[88] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/anshu-kumar-2\n",
            "[89] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/logesh-kumar-umapathi\n",
            "[90] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/ayush-thakur-2\n",
            "[91] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/anuj-saini-2\n",
            "[92] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/nikhil-rana-2\n",
            "[93] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/daksh-varshneya\n",
            "[94] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/hitesh-nayak\n",
            "[95] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/nikhil-kumar-mishra\n",
            "[96] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/saurav-agarwal\n",
            "[97] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/pradeep-kumar\n",
            "[98] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/avinash-pathak-2\n",
            "[99] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/manpreet-singh\n",
            "[100] Crawling: https://www.analyticsvidhya.com/datahacksummit-2025/speakers/shivaraj-mulimani\n",
            "\n",
            "✅ 98 pages crawled after filtering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Further Manual Filtering"
      ],
      "metadata": {
        "id": "VnW9tlxn2ZtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_urls = ['https://www.analyticsvidhya.com/datahacksummit-2025/sessions/llmops-productionalizing-real-world-applications-with-llms-2',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/mastering-real-world-agentic-ai-applications-with-ag2-autogen',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/build-a-production-ready-multi-agent-application-with-crewai',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/from-theory-to-practice-training-llms-reinforcement-learning-and-ai',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/building-a-multimodal-telegram-agent-that-sees-talks-and-thinks',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/generative-ai-for-business-leaders',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/agentops-building-and-deploying-ai-agents',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/agentic-rag-workshop-from-fundamentals-to-real-world-implemenno-title',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/mastering-intelligent-agents-a-deep-dive-into-agentic-ai-building-ai',\n",
        "'https://www.analyticsvidhya.com/datahacksummit-2025/sessions/mastering-llms-training-fine-tuning-and-best-practices-2']\n",
        "\n",
        "filtered_page_data = [\n",
        "    entry for entry in page_data\n",
        "    if entry['url'] == \"https://www.analyticsvidhya.com/datahacksummit\"\n",
        "    or entry['url'].startswith(\"https://www.analyticsvidhya.com/datahacksummit-2025/\")\n",
        "]\n",
        "\n",
        "\n",
        "filtered_page_data = [entry for entry in filtered_page_data if entry['url'] not in remove_urls]\n",
        "\n",
        "print(f\"\\n✅ {len(filtered_page_data)} crawled pages selected after manual filtering.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_W9SXCT2gz7",
        "outputId": "3a53b695-da5e-4845-dfd2-57095a49fb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 80 crawled pages selected after manual filtering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Convert Crawled HTML to Markdown\n",
        "\n",
        "Use **html2text** to transform page HTML to markdown for easier interpretation\n"
      ],
      "metadata": {
        "id": "MpdV3aney3ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all HTML contents to markdown (if converter is available)\n",
        "import html2text\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def convert_html_to_markdown():\n",
        "    \"\"\"Creates and configures an html2text converter.\"\"\"\n",
        "    converter = html2text.HTML2Text()\n",
        "    converter.ignore_links = False       # Set True if you want to remove links\n",
        "    converter.ignore_images = True       # Set False if you want to include image URLs\n",
        "    converter.body_width = 0             # Prevents line wrapping\n",
        "    return converter\n",
        "\n",
        "def extract_section(text, start_keyword=None, end_keywords=None):\n",
        "    \"\"\"\n",
        "    Extracts text between a start keyword and one of several end keywords.\n",
        "\n",
        "    Args:\n",
        "        text: The input string.\n",
        "        start_keyword: The keyword to start extraction after.\n",
        "        end_keywords: A list of keywords to end extraction before.\n",
        "\n",
        "    Returns:\n",
        "        The extracted text, or the original text if keywords are not found.\n",
        "    \"\"\"\n",
        "    if start_keyword:\n",
        "        parts = text.split(start_keyword, maxsplit=1)\n",
        "        text = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "    if end_keywords:\n",
        "        for keyword in end_keywords:\n",
        "            parts = text.split(keyword, maxsplit=1)\n",
        "            text = parts[0].strip() if len(parts) > 1 else text.strip()\n",
        "    return text\n",
        "\n",
        "cleaned_page_content = []\n",
        "converter = convert_html_to_markdown()\n",
        "\n",
        "end_keywords = [\"Participate\", \"Keynote\", \"Workshop Details\", \"Past Edition\", \"Download Brochure\"]\n",
        "\n",
        "for entry in filtered_page_data:\n",
        "    markdown = converter.handle(entry[\"content\"])\n",
        "    cleaned_text = extract_section(markdown, start_keyword=\"Download Brochure\", end_keywords=end_keywords)\n",
        "\n",
        "    cleaned_page_content.append({\n",
        "        \"url\": entry[\"url\"],\n",
        "        \"content\": cleaned_text\n",
        "    })"
      ],
      "metadata": {
        "id": "xB5emo4Ey5gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Save and Inspect Results\n"
      ],
      "metadata": {
        "id": "NzLATLUxy82U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save for reuse\n",
        "with open(\"page_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump(cleaned_page_content, f)"
      ],
      "metadata": {
        "id": "Fk8mm0dJy9wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick summary: crawled URLs and markdown preview\n",
        "print(f\"Total pages: {len(cleaned_page_content)}\")\n",
        "print(\"--- Example URLs ---\")\n",
        "for page in cleaned_page_content[:5]:\n",
        "    print(page['url'])\n",
        "\n",
        "print(\"\\n--- Markdown Snippet from First Page ---\")\n",
        "print(cleaned_page_content[0]['markdown'][:500] if cleaned_page_content and 'markdown' in cleaned_page_content[0] else cleaned_page_content[0]['content'][:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm7HVgQHy_wd",
        "outputId": "9aedf52c-d60a-42c3-eea7-0b8d070cc834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 80\n",
            "--- Example URLs ---\n",
            "https://www.analyticsvidhya.com/datahacksummit\n",
            "https://www.analyticsvidhya.com/datahacksummit-2025/awards\n",
            "https://www.analyticsvidhya.com/datahacksummit-2025/workshops/mastering-intelligent-agents-a-deep-dive-into-agentic-ai-building-ai\n",
            "https://www.analyticsvidhya.com/datahacksummit-2025/workshops/build-a-production-ready-multi-agent-application-with-crewai\n",
            "https://www.analyticsvidhya.com/datahacksummit-2025/workshops/agentic-rag-workshop-from-fundamentals-to-real-world-implemenno-title\n",
            "\n",
            "--- Markdown Snippet from First Page ---\n",
            "# The AI Trinity\n",
            "\n",
            "## Powering the Future\n",
            "\n",
            "### Generative | Agentic | Responsible\n",
            "\n",
            "## India's Most Futuristic AI Conference\n",
            "\n",
            "### August 20-23, 2025  • The Leela Bhartiya City Bengaluru \n",
            "\n",
            "Send WhatsApp Updates\n",
            "\n",
            "Download Agenda\n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "## 1200+\n",
            "\n",
            "### Attendees\n",
            "\n",
            "## 80+\n",
            "\n",
            "### AI Talks\n",
            "\n",
            "## 50+\n",
            "\n",
            "### Hack Sessions\n",
            "\n",
            "## 10+\n",
            "\n",
            "### Workshops\n",
            "\n",
            "## Hands-on AI Workshops\n",
            "\n",
            "### [Mastering Intelligent Agents: A Deep Dive into Agentic AI Dipanjan Sarkar - Analytics Vidhya  ](/da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdBDdtEBwl8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}